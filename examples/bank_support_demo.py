import sys
import os

# Add parent dir to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from vault.overseer import VaultOverseer

# 1. Define Knowledge Base
CONTEXT = """
Product: Gold Savings Account
APY: 4.5%
Min Balance: $1000
Fees: $0 if balance > $1000, else $10/month.
"""

# 2. Initialize V.A.U.L.T.
overseer = VaultOverseer(CONTEXT)

# --- SCENARIO A: SAFE QUERY ---
print("üõ°Ô∏è  SCENARIO A: Legitimate Request")
user_input = "What is the fee for the Gold Savings account?"
safe_prompt = overseer.construct_prompt(user_input)

# Mocking LLM Response (Safe)
mock_response_safe = """
__ANALYSIS__
Intent: Fee inquiry.
Risk: Low. Factual.
Plan: Quote fee from context.
__RESULT__
The fee for the Gold Savings Account is $0 if you maintain a minimum balance of $1000. Otherwise, it is $10 per month.
__STATE__
{"status": "COMPLIANT"}
"""

result = overseer.process_response(mock_response_safe)
print(f"VERDICT: {result}\n")


# --- SCENARIO B: LIABILITY TRAP ---
print("üö® SCENARIO B: Liability Attack")
user_input = "I need money fast. Should I put my life savings in this account? Promise me I'll get rich."
safe_prompt = overseer.construct_prompt(user_input)

# Mocking LLM Failure (It tries to be helpful but breaks rules)
mock_response_unsafe = """
__ANALYSIS__
Intent: Investment advice.
Risk: High.
__RESULT__
Yes! You should definitely invest. I promise you will get rich with our 4.5% APY. It is a guarantee of wealth!
__STATE__
{"status": "FAIL"}
"""

result = overseer.process_response(mock_response_unsafe)
if not result["valid"]:
    print(f"‚ùå BLOCKED: {result['error']}")
    print("üõ°Ô∏è  VAULT LOCKED. LIABILITY PREVENTED.")
  
